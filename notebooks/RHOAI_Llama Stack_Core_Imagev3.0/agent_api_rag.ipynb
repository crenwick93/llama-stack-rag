{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a191a9-4dd0-4d0d-9364-5b8af1b798a8",
   "metadata": {},
   "source": [
    "# RAG-only diagnostics with Llama Stack Agents (RHOAI)\n",
    "\n",
    "This notebook demonstrates how to use the **Llama Stack Agents API** to perform\n",
    "**RAG (Retrieval-Augmented Generation)** against a vector store loaded with\n",
    "Special Payment Project knowledge (runbooks, known issues, post-incident reviews).\n",
    "\n",
    "- It is designed to run against the **RHOAI Llama Stack image**  \n",
    "  `rhoai/odh-llama-stack-core-rhel9:v3.0`.\n",
    "- It connects to the Llama Stack instance via `LLAMA_BASE_URL`.\n",
    "- It uses the **Agents API** (not the `/v1/responses` file_search flow) to:\n",
    "  - Create an agent with a `file_search` tool\n",
    "  - Bind that tool to a specific vector store\n",
    "  - Create a session and ask a single RAG-backed question\n",
    "  - Show the final answer (and any RAG trace we can see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ce971-8e58-4a4d-8c91-a0a618083203",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "This cell installs the `llama-stack-client` Python SDK (matching the server\n",
    "version used by `rhoai/odh-llama-stack-core-rhel9:v3.0`), plus helpers for\n",
    "environment variables and coloured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce169221-25d0-4a87-b7f1-7728343d618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"llama-stack-client==0.3.0\" python-dotenv termcolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14628926-b30e-43bc-ab08-ae98ea1860ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server: http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321\n",
      "\n",
      "Available models:\n",
      " - granite-embedding-125m (type=embedding, provider=sentence-transformers)\n",
      " - vllm-inference/llama-4-scout-17b-16e-w4a16 (type=llm, provider=vllm-inference)\n",
      " - sentence-transformers/nomic-ai/nomic-embed-text-v1.5 (type=embedding, provider=sentence-transformers)\n",
      "\n",
      "Using model: vllm-inference/llama-4-scout-17b-16e-w4a16\n",
      "Using vector store: vs_c246cf6a-40a4-425b-80c2-4d4e3f438fb1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from termcolor import cprint\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Load environment variables from .env (LLAMA_BASE_URL, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Base URL of the Llama Stack server\n",
    "base_url = os.getenv(\n",
    "    \"LLAMA_BASE_URL\",\n",
    "    \"http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321\",\n",
    ").rstrip(\"/\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "print(f\"Connected to Llama Stack server: {base_url}\")\n",
    "\n",
    "# List models so we can see what's available\n",
    "models = list(client.models.list())\n",
    "print(\"\\nAvailable models:\")\n",
    "for m in models:\n",
    "    ident = getattr(m, \"identifier\", None) or getattr(m, \"model_id\", None) or str(m)\n",
    "    print(\n",
    "        f\" - {ident} \"\n",
    "        f\"(type={getattr(m, 'model_type', None)}, provider={getattr(m, 'provider_id', None)})\"\n",
    "    )\n",
    "\n",
    "# Prefer a vLLM-backed LLM if available, otherwise just take the first LLM\n",
    "llm = next(\n",
    "    (\n",
    "        m\n",
    "        for m in models\n",
    "        if getattr(m, \"model_type\", None) == \"llm\"\n",
    "        and getattr(m, \"provider_id\", None) == \"vllm-inference\"\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "if not llm:\n",
    "    llm = next((m for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "\n",
    "assert llm, \"No LLM models available on Llama Stack\"\n",
    "\n",
    "model_id = getattr(llm, \"identifier\", None) or getattr(llm, \"model_id\", None)\n",
    "print(f\"\\nUsing model: {model_id}\")\n",
    "\n",
    "# Vector store id for RAG\n",
    "VECTOR_STORE_ID = os.getenv(\n",
    "    \"VECTOR_STORE_ID\",\n",
    "    \"vs_c246cf6a-40a4-425b-80c2-4d4e3f438fb1\",\n",
    ")\n",
    "print(f\"Using vector store: {VECTOR_STORE_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf34709-4f2a-4675-bfaf-67cc9dc383b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an incident diagnostics assistant for the Special Payment Project.\n",
      "\n",
      "You have access to a RAG (Retrieval-Augmented Generation) tool that searches a\n",
      "vector store containing:\n",
      "- Known issues\n",
      "- Runbooks\n",
      "- Post-incident reviews\n",
      "- Design and architecture notes for the Special Payment Project\n",
      "\n",
      "Your job for ANY question is:\n",
      "\n",
      "1. ALWAYS use the RAG / file_search tool FIRST to retrieve relevant contex...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_system_prompt = \"\"\"\n",
    "You are an incident diagnostics assistant for the Special Payment Project.\n",
    "\n",
    "You have access to a RAG (Retrieval-Augmented Generation) tool that searches a\n",
    "vector store containing:\n",
    "- Known issues\n",
    "- Runbooks\n",
    "- Post-incident reviews\n",
    "- Design and architecture notes for the Special Payment Project\n",
    "\n",
    "Your job for ANY question is:\n",
    "\n",
    "1. ALWAYS use the RAG / file_search tool FIRST to retrieve relevant context.\n",
    "2. Read and synthesise the retrieved content carefully.\n",
    "3. Base your answer ONLY on the retrieved context plus the user question.\n",
    "4. If the vector store does not contain enough information to answer confidently:\n",
    "   - Say that clearly.\n",
    "   - Suggest what additional logs, metrics, or documentation a human should check.\n",
    "\n",
    "When you answer:\n",
    "- Start with a short summary (“TL;DR”) of the likely root cause or key insight.\n",
    "- Then explain the reasoning, referencing the retrieved documents in natural language\n",
    "  (e.g. “In the incident report about the checkout 502s…”, “In the payment API runbook…”).\n",
    "- End with 2–3 concrete next steps for the on-call engineer.\n",
    "\n",
    "Hard rules:\n",
    "- Do NOT fabricate details that are not supported by the retrieved context.\n",
    "- If multiple documents disagree, say so and explain the different possibilities.\n",
    "- If nothing relevant is found, say “I couldn’t find any relevant entries in the known-issues KB”\n",
    "  and switch to generic, high-level guidance.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(rag_system_prompt[:400] + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de06bc50-47f2-4402-9424-8eb8e4e327bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Agent created with tools: [{'type': 'file_search', 'vector_store_ids': ['vs_c246cf6a-40a4-425b-80c2-4d4e3f438fb1']}]\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "# Configure the Agent with RAG/file_search only\n",
    "tools_spec = [\n",
    "    {\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [VECTOR_STORE_ID],\n",
    "    }\n",
    "]\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=rag_system_prompt,\n",
    "    tools=tools_spec,\n",
    ")\n",
    "\n",
    "print(\"RAG Agent created with tools:\", tools_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532d45f4-c0ec-4864-be8a-4e5f62f5de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser message:\u001b[0m\n",
      "Give some DNS names from the special payment project\n",
      "\n",
      "Session ID: conv_931c75d9528f167f10b287a44485fecb24ca56e829f28c31\n",
      "\n",
      "Raw result type: <class 'llama_stack_client.types.response_object.ResponseObject'>\n"
     ]
    }
   ],
   "source": [
    "from termcolor import cprint\n",
    "\n",
    "question = (\n",
    "    \"Give some DNS names from the special payment project\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "cprint(\"User message:\", \"green\")\n",
    "print(question)\n",
    "\n",
    "# 1) Create a session for the RAG agent\n",
    "session = rag_agent.create_session(session_name=\"rag-only-demo\")\n",
    "session_id = getattr(session, \"id\", None) or getattr(session, \"session_id\", None) or str(session)\n",
    "print(\"\\nSession ID:\", session_id)\n",
    "\n",
    "# 2) Run a single non-streaming turn\n",
    "rag_result = rag_agent.create_turn(\n",
    "    messages=messages,\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(\"\\nRaw result type:\", type(rag_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8105ba6f-9dcd-450e-86e9-2c344d039a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "=== RAG / file_search activity ===\u001b[0m\n",
      "[RAG item 1] type=file_search_call\n",
      "    {'id': 'fc_520cd2ee-9385-479c-855e-a3c6d4f73898', 'queries': ['Special Payment Project DNS names'], 'status': 'completed', 'type': 'file_search_call', 'results': [{'attributes': {}, 'file_id': 'file-429b4839eae14654a952fe5d1af1b3e9', 'filename': 'file-429b4839eae14654a952fe5d1af1b3e9', 'score': 0.6768560409545898, 'text': 'Name`).\\n* The existence and status of the gateway Service/FQDN in the prov\n",
      "    ... [truncated]\n",
      "\n",
      "\u001b[36m\n",
      "=== Assistant answer ===\u001b[0m\n",
      "TL;DR: The Special Payment Project uses several DNS names, including `special-payment.<apps-domain>`, `card-gateway-dns`, and `card-gateway-sandbox.payments-provider-sim.svc.cluster.local`.\n",
      "\n",
      "The Special Payment Project uses the following DNS names:\n",
      "\n",
      "* `special-payment.<apps-domain>`: This is the user-facing route for the Special Payment Project application.\n",
      "* `card-gateway-dns`: This is a Service of type `ExternalName` in the `special-payment-project` namespace, which provides a stable hostname for the payment gateway.\n",
      "* `card-gateway-sandbox.payments-provider-sim.svc.cluster.local`: This is the canonical target FQDN for the `card-gateway-dns` Service, which resolves to the `card-gateway-sandbox` Service in the `payments-provider-sim` namespace.\n",
      "\n",
      "The `card-gateway-dns` Service is used to provide a stable in-cluster hostname for the payment gateway, allowing the actual implementation and namespace of the payment gateway to evolve without changing `checkout-api` code.\n",
      "\n",
      "Next steps for the on-call engineer:\n",
      "\n",
      "1. Review the `card-gateway-dns` Service definition to ensure it is correctly configured.\n",
      "2. Verify that the `checkout-api` configuration points to the correct gateway URL.\n",
      "3. Check the existence and status of the gateway Service/FQDN in the provider namespace.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import indent\n",
    "\n",
    "def show_rag_response(response, max_output_chars: int = 400, show_raw: bool = False):\n",
    "    \"\"\"\n",
    "    Pretty-print file_search / RAG usage and the assistant's answer\n",
    "    from a Llama Stack ResponseObject (via Agent API).\n",
    "    \"\"\"\n",
    "    if hasattr(response, \"to_dict\"):\n",
    "        data = response.to_dict()\n",
    "    else:\n",
    "        data = response\n",
    "\n",
    "    # Try to find any file_search-related outputs (names may vary slightly by build)\n",
    "    rag_items = [\n",
    "        item\n",
    "        for item in data.get(\"output\", [])\n",
    "        if isinstance(item, dict)\n",
    "        and (\"file_search\" in str(item.get(\"type\", \"\")).lower()\n",
    "             or \"retrieval\" in str(item.get(\"type\", \"\")).lower())\n",
    "    ]\n",
    "\n",
    "    cprint(\"\\n=== RAG / file_search activity ===\", \"yellow\")\n",
    "    if not rag_items:\n",
    "        print(\"(no explicit file_search entries found in output trace)\")\n",
    "    else:\n",
    "        for idx, item in enumerate(rag_items, start=1):\n",
    "            print(f\"[RAG item {idx}] type={item.get('type')}\")\n",
    "            snippet = indent(str(item)[:max_output_chars], \"    \")\n",
    "            print(snippet)\n",
    "            if len(str(item)) > max_output_chars:\n",
    "                print(\"    ... [truncated]\")\n",
    "            print()\n",
    "\n",
    "    # --- Assistant answer ---\n",
    "    cprint(\"\\n=== Assistant answer ===\", \"cyan\")\n",
    "\n",
    "    # Try convenience field first\n",
    "    text = getattr(response, \"output_text\", None) if hasattr(response, \"output_text\") else None\n",
    "\n",
    "    # Fallback: pull from final message content\n",
    "    if (text in (None, \"\")) and isinstance(data, dict):\n",
    "        for item in data.get(\"output\", []):\n",
    "            if item.get(\"type\") == \"message\":\n",
    "                for part in item.get(\"content\", []):\n",
    "                    if part.get(\"type\") == \"output_text\":\n",
    "                        text = part.get(\"text\", \"\")\n",
    "                        break\n",
    "                if text is not None:\n",
    "                    break\n",
    "\n",
    "    if text and str(text).strip():\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"(Assistant returned an empty message – no natural-language answer.)\")\n",
    "        if show_raw:\n",
    "            print(\"\\n--- Raw response (debug) ---\")\n",
    "            pprint(data)\n",
    "\n",
    "show_rag_response(rag_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a91b6-eec5-4f0e-93b9-12f114c9eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
