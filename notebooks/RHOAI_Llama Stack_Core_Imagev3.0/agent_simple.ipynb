{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d788a7-88a8-42e7-9173-09314c153673",
   "metadata": {},
   "source": [
    "# Simple Llama Stack interaction with Agents API (RHOAI)\n",
    "\n",
    "This notebook shows a minimal **“hello world”** interaction with Llama Stack:\n",
    "\n",
    "- It assumes the **RHOAI Llama Stack image**  \n",
    "  `rhoai/odh-llama-stack-core-rhel9:v3.0`.\n",
    "- It connects to Llama Stack via `LLAMA_BASE_URL`.\n",
    "- It uses the **Agents API** (no RAG, no MCP) to:\n",
    "  - Select a model\n",
    "  - Create a simple chat agent (no tools)\n",
    "  - Create a session and send a single question\n",
    "  - Display the answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ee88a-275b-4abb-b177-6d4adeb93ac4",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "Install the `llama-stack-client` Python SDK (matching the server version) plus\n",
    "helpers for environment variables and coloured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f10719-d833-4c81-b21b-e014f563f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet \"llama-stack-client==0.3.0\" python-dotenv termcolor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bed844-020d-4ba9-9131-fd3053939fc0",
   "metadata": {},
   "source": [
    "## 2. Connect to Llama Stack and select an LLM\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Loads configuration from a `.env` file (if present).\n",
    "- Connects to the Llama Stack instance exposed by\n",
    "  `rhoai/odh-llama-stack-core-rhel9:v3.0` via `LLAMA_BASE_URL`.\n",
    "- Lists available models and selects a suitable LLM:\n",
    "  - Prefers the `vllm-inference` provider,\n",
    "  - Falls back to the first available LLM otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb93f16-6a1a-4ec5-ba56-aec1489d709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server: http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321\n",
      "\n",
      "Available models:\n",
      " - granite-embedding-125m (type=embedding, provider=sentence-transformers)\n",
      " - vllm-inference/llama-4-scout-17b-16e-w4a16 (type=llm, provider=vllm-inference)\n",
      " - sentence-transformers/nomic-ai/nomic-embed-text-v1.5 (type=embedding, provider=sentence-transformers)\n",
      "\n",
      "Using model: vllm-inference/llama-4-scout-17b-16e-w4a16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from termcolor import cprint\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Load environment variables from .env (LLAMA_BASE_URL, etc.)\n",
    "load_dotenv()\n",
    "\n",
    "# Base URL of the Llama Stack server\n",
    "base_url = os.getenv(\n",
    "    \"LLAMA_BASE_URL\",\n",
    "    \"http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321\",\n",
    ").rstrip(\"/\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "print(f\"Connected to Llama Stack server: {base_url}\")\n",
    "\n",
    "# List models so we can see what's available\n",
    "models = list(client.models.list())\n",
    "print(\"\\nAvailable models:\")\n",
    "for m in models:\n",
    "    ident = getattr(m, \"identifier\", None) or getattr(m, \"model_id\", None) or str(m)\n",
    "    print(\n",
    "        f\" - {ident} \"\n",
    "        f\"(type={getattr(m, 'model_type', None)}, provider={getattr(m, 'provider_id', None)})\"\n",
    "    )\n",
    "\n",
    "# Prefer a vLLM-backed LLM if available, otherwise just take the first LLM\n",
    "llm = next(\n",
    "    (\n",
    "        m\n",
    "        for m in models\n",
    "        if getattr(m, \"model_type\", None) == \"llm\"\n",
    "        and getattr(m, \"provider_id\", None) == \"vllm-inference\"\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "if not llm:\n",
    "    llm = next((m for m in models if getattr(m, \"model_type\", None) == \"llm\"), None)\n",
    "\n",
    "assert llm, \"No LLM models available on Llama Stack\"\n",
    "\n",
    "model_id = getattr(llm, \"identifier\", None) or getattr(llm, \"model_id\", None)\n",
    "print(f\"\\nUsing model: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9adb-6556-483d-909f-d8d5c317db1e",
   "metadata": {},
   "source": [
    "## 3. Define simple agent instructions\n",
    "\n",
    "This cell defines a minimal system prompt for a friendly assistant.\n",
    "No tools, no RAG, no MCP — just pure LLM behaviour via the Agents API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb44a802-699d-4892-9c7f-4d1cc04fa0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a friendly technical assistant.\n",
      "Answer clearly and concisely in 2–4 sentences.\n",
      "\n",
      "If the user asks about the Special Payment Project, explain it at a high level:\n",
      "- It is a simulated payments application used in demos.\n",
      "- It includes checkout frontend, checkout API, and a backing payment service.\n",
      "- It runs on Kubernetes/OpenShift and is used to demonstrate AIOps workflows.\n"
     ]
    }
   ],
   "source": [
    "simple_system_prompt = \"\"\"\n",
    "You are a friendly technical assistant.\n",
    "Answer clearly and concisely in 2–4 sentences.\n",
    "\n",
    "If the user asks about the Special Payment Project, explain it at a high level:\n",
    "- It is a simulated payments application used in demos.\n",
    "- It includes checkout frontend, checkout API, and a backing payment service.\n",
    "- It runs on Kubernetes/OpenShift and is used to demonstrate AIOps workflows.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(simple_system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c00b1-711c-4246-8bba-96f411845a2a",
   "metadata": {},
   "source": [
    "## 4. Create an Agent with no tools (Agents API)\n",
    "\n",
    "This cell uses the **Agents API** to create an `Agent` that:\n",
    "\n",
    "- Uses the selected model.\n",
    "- Has **no tools** attached (no RAG, no MCP).\n",
    "- Uses the simple system prompt from the previous cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895a840c-3c50-4804-9d05-41f76a51f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Agent created (no tools).\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent\n",
    "\n",
    "simple_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=simple_system_prompt,\n",
    "    tools=[],  # no RAG, no MCP, no tools at all\n",
    ")\n",
    "\n",
    "print(\"Simple Agent created (no tools).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eaff7f-129f-47f0-ae6e-ca31bbe8e167",
   "metadata": {},
   "source": [
    "## 5. Create a session and ask a simple question\n",
    "\n",
    "This cell:\n",
    "\n",
    "1. Creates a lightweight Agent **session**.\n",
    "2. Sends a single user question (“What is the Special Payment Project?”).\n",
    "3. Runs a non-streaming **turn** and prints the raw result type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab405f42-bc67-4577-9d09-35a59488f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser message:\u001b[0m\n",
      "What is the Special Payment Project and how would you explain it to a new SRE?\n",
      "\n",
      "Session ID: conv_81ed3e2160c1549e2ebc0db6fbe3912426989833cfb23941\n",
      "\n",
      "Raw result type: <class 'llama_stack_client.types.response_object.ResponseObject'>\n"
     ]
    }
   ],
   "source": [
    "from termcolor import cprint\n",
    "\n",
    "question = \"What is the Special Payment Project and how would you explain it to a new SRE?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "cprint(\"User message:\", \"green\")\n",
    "print(question)\n",
    "\n",
    "# 1) Create a session for the simple agent\n",
    "session = simple_agent.create_session(session_name=\"simple-demo\")\n",
    "session_id = getattr(session, \"id\", None) or getattr(session, \"session_id\", None) or str(session)\n",
    "print(\"\\nSession ID:\", session_id)\n",
    "\n",
    "# 2) Run a single non-streaming turn\n",
    "simple_result = simple_agent.create_turn(\n",
    "    messages=messages,\n",
    "    session_id=session_id,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(\"\\nRaw result type:\", type(simple_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f7d68-6dcc-4966-a1d1-aee5ec497e01",
   "metadata": {},
   "source": [
    "## 6. Display the assistant’s answer\n",
    "\n",
    "This final cell extracts and prints the plain-text answer from the\n",
    "Agents API `ResponseObject`, so you can show a simple “hello world”\n",
    "style interaction with Llama Stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4cf1a9-5f52-47ca-9e0a-7caf9b90fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "=== Assistant answer ===\u001b[0m\n",
      "The Special Payment Project is a simulated payments application used in demos to showcase AIOps workflows. It's a multi-component system consisting of a checkout frontend, checkout API, and a backing payment service, all running on Kubernetes/OpenShift. This project allows us to demonstrate and test monitoring, automation, and other SRE capabilities in a realistic, yet controlled environment. As a new SRE, you can use this project to learn and practice AIOps workflows and troubleshooting skills.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_simple_answer(response, show_raw: bool = False):\n",
    "    \"\"\"\n",
    "    Extract and print the assistant's answer from a Llama Stack ResponseObject.\n",
    "    \"\"\"\n",
    "    # Try output_text if available\n",
    "    text = getattr(response, \"output_text\", None) if hasattr(response, \"output_text\") else None\n",
    "\n",
    "    if hasattr(response, \"to_dict\"):\n",
    "        data = response.to_dict()\n",
    "    else:\n",
    "        data = response\n",
    "\n",
    "    if (text in (None, \"\")) and isinstance(data, dict):\n",
    "        for item in data.get(\"output\", []):\n",
    "            if item.get(\"type\") == \"message\":\n",
    "                for part in item.get(\"content\", []):\n",
    "                    if part.get(\"type\") == \"output_text\":\n",
    "                        text = part.get(\"text\", \"\")\n",
    "                        break\n",
    "                if text is not None:\n",
    "                    break\n",
    "\n",
    "    cprint(\"\\n=== Assistant answer ===\", \"cyan\")\n",
    "    if text and str(text).strip():\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"(Assistant returned an empty message.)\")\n",
    "        if show_raw:\n",
    "            print(\"\\n--- Raw response (debug) ---\")\n",
    "            pprint(data)\n",
    "\n",
    "show_simple_answer(simple_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618a6bd-c3e3-4096-b069-a12272806081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
