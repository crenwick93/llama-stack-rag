{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Agent Workbench (Llama Stack)\n",
        "\n",
        "This notebook exercises the agent portion of your RAG workflow against the already running Llama Stack.\n",
        "\n",
        "It mirrors the logic in `agent/app.py`:\n",
        "- Create `LlamaStackClient`\n",
        "- Select an LLM (preferring provider `vllm-inference`)\n",
        "- Resolve vector DB (defaults to `confluence`)\n",
        "- Create an Agent with `builtin::rag/knowledge_search`\n",
        "- Open a session and ask a question (non-streaming; optional streaming cell)\n",
        "\n",
        "Set `LLAMA_BASE_URL` and (optionally) `VECTOR_DB_ID` in the environment, or rely on the defaults used below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet \"llama-stack-client==0.2.23\"\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLAMA_BASE_URL = os.getenv(\"LLAMA_BASE_URL\", \"http://lsd-llama-milvus-inline-service.default.svc.cluster.local:8321\").rstrip(\"/\")\n",
        "VECTOR_DB_ID = os.getenv(\"VECTOR_DB_ID\", \"confluence\")\n",
        "print(\"LLAMA_BASE_URL:\", LLAMA_BASE_URL)\n",
        "print(\"VECTOR_DB_ID:\", VECTOR_DB_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_stack_client import LlamaStackClient, Agent\n",
        "\n",
        "client = LlamaStackClient(base_url=LLAMA_BASE_URL)\n",
        "client\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select an LLM (prefer vllm-inference)\n",
        "models = list(client.models.list())\n",
        "llm = next((m for m in models if m.model_type == \"llm\" and getattr(m, \"provider_id\", None) == \"vllm-inference\"), None)\n",
        "if not llm:\n",
        "    llm = next((m for m in models if m.model_type == \"llm\"), None)\n",
        "assert llm, \"No LLM models available on Llama Stack\"\n",
        "model_id = llm.identifier\n",
        "print(\"Using model:\", model_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resolve vector DB id (prefer the requested one)\n",
        "vdbs = list(client.vector_dbs.list())\n",
        "vector_db_id = next((v.identifier for v in vdbs if getattr(v, \"identifier\", None) == VECTOR_DB_ID), (vdbs[0].identifier if vdbs else VECTOR_DB_ID))\n",
        "print(f\"Vector DB: requested={VECTOR_DB_ID} -> using={vector_db_id}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an Agent bound to the vector DB via builtin RAG tool\n",
        "instructions = (\n",
        "    \"You are a helpful assistant. Use the RAG tool when appropriate and cite source_url(s).\"\n",
        ")\n",
        "rag_agent = Agent(\n",
        "    client,\n",
        "    model=model_id,\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\n",
        "            \"name\": \"builtin::rag/knowledge_search\",\n",
        "            \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "print(\"Agent ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
        "session_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question (non-streaming)\n",
        "question = \"Summarise the resolution for when Disk full on /var. Use our Confluence docs.\"\n",
        "result = rag_agent.create_turn(\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    session_id=session_id,\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "answer = None\n",
        "if isinstance(result, dict):\n",
        "    answer = result.get(\"message\") or result.get(\"content\") or result.get(\"text\")\n",
        "if answer is None and hasattr(result, \"message\"):\n",
        "    answer = getattr(result, \"message\")\n",
        "if answer is None and hasattr(result, \"content\"):\n",
        "    answer = getattr(result, \"content\")\n",
        "\n",
        "print(answer or result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: streaming example\n",
        "from llama_stack_client import AgentEventLogger\n",
        "\n",
        "stream = rag_agent.create_turn(\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    session_id=session_id,\n",
        "    stream=True,\n",
        ")\n",
        "for event in AgentEventLogger().log(stream):\n",
        "    event.print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
