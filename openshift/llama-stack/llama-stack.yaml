apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-providers
  namespace: llama-stack-demo
data:
  providers.yaml: |
    version: 2
    image_name: starter

    apis:
      - inference
      - vector_io

    providers:
      inference:
        - provider_id: vllm-chat
          provider_type: remote::vllm
          config:
            url: https://mistral-small-24b-w8a8-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1
            default_model_id: mistral-small-24b-w8a8
            api_token: ${env.VLLM_CHAT_API_KEY}

        - provider_id: st-embed
          provider_type: inline::sentence-transformers
          config: {}

      vector_io:
        - provider_id: sqlite
          provider_type: inline::sqlite-vec   # ‚Üê hyphenated provider type
          config:
            db_path: /.llama/vec.db
            embedding:
              provider_id: st-embed
              model_id: all-MiniLM-L6-v2

    models:
      - model_id: mistral-small-24b-w8a8
        provider_id: vllm-chat
        provider_model_id: mistral-small-24b-w8a8
        model_type: llm

      - model_id: all-MiniLM-L6-v2
        provider_id: st-embed
        provider_model_id: sentence-transformers/all-MiniLM-L6-v2
        model_type: embedding
        metadata:
          embedding_dimension: 384
          normalize: true

    server:
      port: 8000







---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack
  namespace: llama-stack-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack
  template:
    metadata:
      labels:
        app: llama-stack
    spec:
      enableServiceLinks: false
      containers:
        - name: llama-stack
          image: quay.io/crenwick93/llama-stack:demo
          imagePullPolicy: Always
          command: ["python", "-m", "llama_stack.core.server.server"]
          args: ["/app/config.yaml"]
          env:
            - name: LLAMA_STACK_PORT
              value: "8000"
            - name: VLLM_CHAT_API_KEY
              valueFrom:
                secretKeyRef:
                  name: llama-provider-keys
                  key: VLLM_CHAT_API_KEY
            - name: VLLM_EMBED_API_KEY
              valueFrom:
                secretKeyRef:
                  name: llama-provider-keys
                  key: VLLM_EMBED_API_KEY
          ports:
            - name: http
              containerPort: 8000
          volumeMounts:
            - name: providers
              mountPath: /app/config.yaml
              subPath: providers.yaml
            - name: llama-state
              mountPath: /.llama
      volumes:
        - name: providers
          configMap:
            name: llama-stack-providers
        - name: llama-state
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack
  namespace: llama-stack-demo
spec:
  type: ClusterIP
  selector:
    app: llama-stack
  ports:
    - name: http
      port: 8000
      targetPort: 8000


